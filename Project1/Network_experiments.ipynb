{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with the effect of maxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import dlc_practical_prologue as prologue\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset (reduced set)\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 1, 28, 28])\n",
      "test_input: torch.Size([1000, 1, 28, 28])\n",
      "train_target: torch.Size([1000, 10])\n",
      "test_target: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Visualize dataset dimensions\n",
    "print('train_input:',train_input.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('test_target:',test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 582\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(torch.argmax(train_target[sample])))\n",
    "axis[1].imshow(train_input[sample+1,0])\n",
    "axis[1].set_title('Class: {}'.format(torch.argmax(train_target[sample+1])))\n",
    "plt.show()\n",
    "print('image size:',train_input[sample,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor example\n",
    "class ProfessorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProfessorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from professor's example, without maxPooling\n",
    "class SimplifiedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64*20*20, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*20*20)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    criterion = nn.MSELoss()\n",
    "    eta = 1e-1\n",
    "    for e in range(0, 25):\n",
    "        sum_loss = 0\n",
    "        # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def compute_nb_errors(model, input_, target, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, input_.size(0), mini_batch_size):\n",
    "        output = model(input_.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = target[b:b+mini_batch_size].argmax(1)\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/input_.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9033293426036835\n",
      "1 0.7755392491817474\n",
      "2 0.6989829018712044\n",
      "3 0.6369436867535114\n",
      "4 0.5857527479529381\n",
      "5 0.5477792881429195\n",
      "6 0.5352983623743057\n",
      "7 0.4928423687815666\n",
      "8 0.4678100608289242\n",
      "9 0.4578804410994053\n",
      "10 0.41963087394833565\n",
      "11 0.4123355597257614\n",
      "12 0.3945891596376896\n",
      "13 0.398359015583992\n",
      "14 0.36867518350481987\n",
      "15 0.3492415063083172\n",
      "16 0.35235254652798176\n",
      "17 0.3329454082995653\n",
      "18 0.33431423269212246\n",
      "19 0.3220176585018635\n",
      "20 0.30278079584240913\n",
      "21 0.30568417347967625\n",
      "22 0.2838192079216242\n",
      "23 0.2894000131636858\n",
      "24 0.28004286997020245\n",
      "Loss on test set: 15.299999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Train the professor's model and evaluate it\n",
    "model = ProfessorNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8435735255479813\n",
      "1 0.6591185517609119\n",
      "2 0.5006121657788754\n",
      "3 0.4407782480120659\n",
      "4 0.3672323301434517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f29f3177c928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss on test set:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_nb_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-63d8be793a7e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda2/envs/DeepLearning/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda2/envs/DeepLearning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the simplified model and evaluate it\n",
    "model = SimplifiedNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "nb = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 2, 14, 14])\n",
      "train_target: torch.Size([1000])\n",
      "train_classes: torch.Size([1000, 2])\n",
      "test_input: torch.Size([1000, 2, 14, 14])\n",
      "test_target: torch.Size([1000])\n",
      "test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Display the data shape\n",
    "print('train_input:',train_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('train_classes:',train_classes.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('test_target:',test_target.shape)\n",
    "print('test_classes:',test_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected classification:  0\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 593\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(train_classes[sample,0]))\n",
    "axis[1].imshow(train_input[sample,1])\n",
    "axis[1].set_title('Class: {}'.format(train_classes[sample,1]))\n",
    "plt.show()\n",
    "print('Expected classification: ',train_target[sample].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Define a module responsible for recognizing the classes and another that will identify if the first digit is less or equal to the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Net that works with a single image (hence channel dim = 1)\n",
    "class Parallel_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to classify the digit.\n",
    "\n",
    "        Net input: Nx1x14x14 (single image)\n",
    "        Net output: 10x1\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Parallel_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*3*3, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*3*3)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to infer digit's relation.\n",
    "\n",
    "        Net input: 20x1 (digit classification)\n",
    "        Net output: 1x1 (bigger or smaller)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Analyzer_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2*10, 15)\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.fc4 = nn.Linear(5, 1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)''' # torch.sigmoid() maybe?\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Basic Network with only a MSE loss on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    '''\n",
    "        Net caracteristics:\n",
    "            - No weight sharing\n",
    "            - No intermediate loss\n",
    "        \n",
    "        Net input: Nx2x14x14\n",
    "        Net output: 2x1 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.parallel_net1 = Parallel_Net()\n",
    "        self.parallel_net2 = Parallel_Net()\n",
    "        self.analyser_net  = Analyzer_Net()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Split the 2 input channels\n",
    "        x1 = x[:,0,:,:].view(-1,1,14,14)\n",
    "        x2 = x[:,1,:,:].view(-1,1,14,14)\n",
    "\n",
    "        # No weight sharing (declare 2 distinct instances of Parallel_Net)\n",
    "        x1 = self.parallel_net1(x1)\n",
    "        x2 = self.parallel_net2(x2)\n",
    "\n",
    "        # Concatenate back both classification results \n",
    "        x = torch.cat((x1.view(-1,10),x2.view(-1,10)),dim=1)\n",
    "        x = self.analyser_net(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMSE(model, train_input, train_target, mini_batch_size):\n",
    "    train_target = train_target.type(torch.FloatTensor)\n",
    "\n",
    "    # Specify the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.1\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFinalOutput(model,test_input, test_target,mini_batch_size):\n",
    "    test_target = test_target.type(torch.FloatTensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        error = 0\n",
    "        for b in range(0, test_input.size(0), mini_batch_size):\n",
    "            output = model(test_input.narrow(0, b, mini_batch_size))\n",
    "            for i in range(output.size(0)):\n",
    "                #print(output[i].item(),test_target.narrow(0, b, batch_size)[i].item())\n",
    "                if output[i].item() >= 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() < 0.2:\n",
    "                        error += 1\n",
    "                elif output[i].item() < 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() > 0.8:\n",
    "                        error += 1\n",
    "                else:\n",
    "                    error += 1\n",
    "    return error/test_target.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 99352\n"
     ]
    }
   ],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Create an instance of the network\n",
    "basicModel = BasicNet()\n",
    "num_param = sum(p.numel() for p in basicModel.parameters() if p.requires_grad)\n",
    "print('Number of trainable parameters:',num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 1.2532578706741333\n",
      "Sum of loss at epoch 1: \t 1.2474425435066223\n",
      "Sum of loss at epoch 2: \t 1.2454077452421188\n",
      "Sum of loss at epoch 3: \t 1.24446801841259\n",
      "Sum of loss at epoch 4: \t 1.2439560294151306\n",
      "Sum of loss at epoch 5: \t 1.2436539232730865\n",
      "Sum of loss at epoch 6: \t 1.2434634268283844\n",
      "Sum of loss at epoch 7: \t 1.2433392256498337\n",
      "Sum of loss at epoch 8: \t 1.2432561069726944\n",
      "Sum of loss at epoch 9: \t 1.24319888651371\n",
      "Sum of loss at epoch 10: \t 1.2431587278842926\n",
      "Sum of loss at epoch 11: \t 1.243130087852478\n",
      "Sum of loss at epoch 12: \t 1.243108868598938\n",
      "Sum of loss at epoch 13: \t 1.2430929392576218\n",
      "Sum of loss at epoch 14: \t 1.2430806457996368\n",
      "Sum of loss at epoch 15: \t 1.243071049451828\n",
      "Sum of loss at epoch 16: \t 1.243063360452652\n",
      "Sum of loss at epoch 17: \t 1.243057206273079\n",
      "Sum of loss at epoch 18: \t 1.243052139878273\n",
      "Sum of loss at epoch 19: \t 1.2430478632450104\n",
      "Sum of loss at epoch 20: \t 1.243044227361679\n",
      "Sum of loss at epoch 21: \t 1.2430410087108612\n",
      "Sum of loss at epoch 22: \t 1.2430382072925568\n",
      "Sum of loss at epoch 23: \t 1.2430356591939926\n",
      "Sum of loss at epoch 24: \t 1.2430333495140076\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "basicModel = trainMSE(basicModel,train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  44.3 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "res = evaluateFinalOutput(basicModel,test_input,test_target,mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cross-entropy loss at the classification level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Distinction between `clone()` and `copy_()`\n",
    "Both functions allow to create a clone of a Tensor instance. The difference is that using `clone()`, this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor!\n",
    "Therefore `copy_()` allows to fully separate the cloned tensor from its original version and makes it independent.\n",
    "\n",
    "Check [the doc](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone) for more details (`copy_` is right below `clone`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train each submodule individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Class Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassIdentifier(model, train_input, train_classes, mini_batch_size):\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 50\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.01\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            for i in range(2):\n",
    "                # For each image (since there are 2 channels)\n",
    "                output = model(train_input[:,i].view(-1,1,14,14).narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "                loss = criterion(output, train_classes[:,i].narrow(0, b, mini_batch_size))\n",
    "                sum_loss += loss.item()\n",
    "                loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def evaluateClassIdentification(model, test_input, test_classes, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input[:,0].view(-1,1,14,14).narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = test_classes[:,0][b:b+mini_batch_size]\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/test_input.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Instantiate the model\n",
    "classModel = Parallel_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 31.49049735069275\n",
      "Sum of loss at epoch 1: \t 20.087369918823242\n",
      "Sum of loss at epoch 2: \t 17.723151803016663\n",
      "Sum of loss at epoch 3: \t 14.297038674354553\n",
      "Sum of loss at epoch 4: \t 13.88499540090561\n",
      "Sum of loss at epoch 5: \t 9.506675243377686\n",
      "Sum of loss at epoch 6: \t 9.820306718349457\n",
      "Sum of loss at epoch 7: \t 6.737525641918182\n",
      "Sum of loss at epoch 8: \t 6.035516083240509\n",
      "Sum of loss at epoch 9: \t 5.872492611408234\n",
      "Sum of loss at epoch 10: \t 4.430459260940552\n",
      "Sum of loss at epoch 11: \t 4.175244510173798\n",
      "Sum of loss at epoch 12: \t 4.4085473120212555\n",
      "Sum of loss at epoch 13: \t 2.96647872030735\n",
      "Sum of loss at epoch 14: \t 3.9429188072681427\n",
      "Sum of loss at epoch 15: \t 2.5074550062417984\n",
      "Sum of loss at epoch 16: \t 2.259713441133499\n",
      "Sum of loss at epoch 17: \t 2.905395805835724\n",
      "Sum of loss at epoch 18: \t 2.0890191048383713\n",
      "Sum of loss at epoch 19: \t 1.6869547367095947\n",
      "Sum of loss at epoch 20: \t 1.506991170346737\n",
      "Sum of loss at epoch 21: \t 1.4606547206640244\n",
      "Sum of loss at epoch 22: \t 1.3788674995303154\n",
      "Sum of loss at epoch 23: \t 3.3489713072776794\n",
      "Sum of loss at epoch 24: \t 1.5751380175352097\n",
      "Sum of loss at epoch 25: \t 1.1704645827412605\n",
      "Sum of loss at epoch 26: \t 1.04196148365736\n",
      "Sum of loss at epoch 27: \t 0.9565146788954735\n",
      "Sum of loss at epoch 28: \t 0.8770140632987022\n",
      "Sum of loss at epoch 29: \t 0.8130037263035774\n",
      "Sum of loss at epoch 30: \t 0.7542530074715614\n",
      "Sum of loss at epoch 31: \t 0.6997553445398808\n",
      "Sum of loss at epoch 32: \t 0.6491203606128693\n",
      "Sum of loss at epoch 33: \t 0.60939871519804\n",
      "Sum of loss at epoch 34: \t 0.5663555338978767\n",
      "Sum of loss at epoch 35: \t 0.5301939398050308\n",
      "Sum of loss at epoch 36: \t 0.4927305653691292\n",
      "Sum of loss at epoch 37: \t 0.45561591908335686\n",
      "Sum of loss at epoch 38: \t 0.41481160931289196\n",
      "Sum of loss at epoch 39: \t 0.3901264015585184\n",
      "Sum of loss at epoch 40: \t 0.3542248550802469\n",
      "Sum of loss at epoch 41: \t 0.33256470784544945\n",
      "Sum of loss at epoch 42: \t 0.307589128613472\n",
      "Sum of loss at epoch 43: \t 0.29713355004787445\n",
      "Sum of loss at epoch 44: \t 0.2698528841137886\n",
      "Sum of loss at epoch 45: \t 0.2555000390857458\n",
      "Sum of loss at epoch 46: \t 0.23729989118874073\n",
      "Sum of loss at epoch 47: \t 0.22429984994232655\n",
      "Sum of loss at epoch 48: \t 0.2094810027629137\n",
      "Sum of loss at epoch 49: \t 0.19988710526376963\n"
     ]
    }
   ],
   "source": [
    "classModel = trainClassIdentifier(classModel, train_input, train_classes, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  6.3 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the class identification of the model\n",
    "res = evaluateClassIdentification(classModel, test_input, test_classes, mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAnalyzer(model, train_classes, train_target, mini_batch_size):\n",
    "    train_target = train_target.type(torch.FloatTensor)\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 1e-0\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    \n",
    "    # One hot encode the training classes and concatenate them (1000x2)->(1000x2x10)\n",
    "    train_oneHot = torch.empty([train_classes.shape[0],2,10])\n",
    "    train_oneHot[:,0] = torch.eye(10)[train_classes[:,0]]\n",
    "    train_oneHot[:,1] = torch.eye(10)[train_classes[:,1]]\n",
    "    # Convert to the format expected by the Analyzer (1000x20)\n",
    "    train_oneHot_cat = torch.cat([train_oneHot[:,0],train_oneHot[:,1]],dim=1)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_oneHot_cat.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model(train_oneHot_cat.narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            '''for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)'''\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifiy One Hot encoding and concatenation\n",
    "train_oneHot = torch.empty([train_classes.shape[0],2,10])\n",
    "train_oneHot[:,0] = torch.eye(10)[train_classes[:,0]]\n",
    "train_oneHot[:,1] = torch.eye(10)[train_classes[:,1]]\n",
    "train_oh_cat = torch.cat([train_oneHot[:,0],train_oneHot[:,1]],dim=1)\n",
    "#print(train_oneHot[0:10,0])\n",
    "#print(train_classes[0:10,0])\n",
    "#print(train_oh_cat[0:10])\n",
    "#print(train_classes[0:10,0],train_classes[0:10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAnalyzer(model,test_classes, test_target,mini_batch_size):\n",
    "    test_target = test_target.type(torch.FloatTensor)\n",
    "    \n",
    "    # One hot encode the training classes and concatenate them (1000x2)->(1000x2x10)\n",
    "    test_oneHot = torch.empty([test_classes.shape[0],2,10])\n",
    "    test_oneHot[:,0] = torch.eye(10)[test_classes[:,0]]\n",
    "    test_oneHot[:,1] = torch.eye(10)[test_classes[:,1]]\n",
    "    # Convert to the format expected by the Analyzer (1000x20)\n",
    "    test_oneHot_cat = torch.cat([test_oneHot[:,0],test_oneHot[:,1]],dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        error = 0\n",
    "        for b in range(0, test_oneHot_cat.size(0), mini_batch_size):\n",
    "            output = model(test_oneHot_cat.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            '''samp = 143\n",
    "            print(output[samp].item())\n",
    "            print(test_target[b+samp].item())\n",
    "            print(test_oneHot_cat.narrow(0, b, mini_batch_size)[samp])\n",
    "            return'''\n",
    "            \n",
    "            for i in range(output.size(0)):\n",
    "                #print(output[i].item(),test_target.narrow(0, b, batch_size)[i].item())\n",
    "                if output[i].item() >= 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() < 0.2:\n",
    "                        error += 1\n",
    "                elif output[i].item() < 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() > 0.8:\n",
    "                        error += 1\n",
    "                else:\n",
    "                    error += 1\n",
    "    return error/test_target.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Instantiate the model\n",
    "analyzerModel = Analyzer_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 1.2460974305868149\n",
      "Sum of loss at epoch 1: \t 1.2443861663341522\n",
      "Sum of loss at epoch 2: \t 1.2446050643920898\n",
      "Sum of loss at epoch 3: \t 1.244727686047554\n",
      "Sum of loss at epoch 4: \t 1.2447603642940521\n",
      "Sum of loss at epoch 5: \t 1.244760051369667\n",
      "Sum of loss at epoch 6: \t 1.2447492629289627\n",
      "Sum of loss at epoch 7: \t 1.2447355687618256\n",
      "Sum of loss at epoch 8: \t 1.2447212785482407\n",
      "Sum of loss at epoch 9: \t 1.244707077741623\n",
      "Sum of loss at epoch 10: \t 1.2446932047605515\n",
      "Sum of loss at epoch 11: \t 1.244679719209671\n",
      "Sum of loss at epoch 12: \t 1.2446666061878204\n",
      "Sum of loss at epoch 13: \t 1.2446539103984833\n",
      "Sum of loss at epoch 14: \t 1.2446415573358536\n",
      "Sum of loss at epoch 15: \t 1.2446295022964478\n",
      "Sum of loss at epoch 16: \t 1.2446178048849106\n",
      "Sum of loss at epoch 17: \t 1.2446064352989197\n",
      "Sum of loss at epoch 18: \t 1.244595393538475\n",
      "Sum of loss at epoch 19: \t 1.2445846199989319\n",
      "Sum of loss at epoch 20: \t 1.2445740848779678\n",
      "Sum of loss at epoch 21: \t 1.2445638477802277\n",
      "Sum of loss at epoch 22: \t 1.244553878903389\n",
      "Sum of loss at epoch 23: \t 1.2445441484451294\n",
      "Sum of loss at epoch 24: \t 1.244534656405449\n"
     ]
    }
   ],
   "source": [
    "analyzerModel = trainAnalyzer(analyzerModel, train_classes, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  44.3 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the relation prediction between the digit of the model\n",
    "res = evaluateAnalyzer(analyzerModel,test_classes,test_target,mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepLearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
