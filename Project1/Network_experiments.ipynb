{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with the effect of maxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import dlc_practical_prologue as prologue\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset (reduced set)\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 1, 28, 28])\n",
      "test_input: torch.Size([1000, 1, 28, 28])\n",
      "train_target: torch.Size([1000, 10])\n",
      "test_target: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Visualize dataset dimensions\n",
    "print('train_input:',train_input.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('test_target:',test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFKFJREFUeJzt3XmYXXV9x/H3hzBJDCEQQDDEkBAJKEQ2h4iCSlUsIgJxQYKltIqxlUUWF6StUPpI0SCCZXtiwQSXIEhUVFAwIEsFJGxCjMqWSCBNWFITRMIk+faPuXk6ze83yZ25y9z7m8/refLcme/9nXO+Z+Y735x7VkUEZmbW/jYb6ATMzKw+3NDNzArhhm5mVgg3dDOzQrihm5kVwg3dzKwQbugNJulsSd8e6DzM6s213Xrc0OtA0jGS5kt6UdJSSTdKOnCg8wKQNEHSrZJekvQ7Se8e6JysfbR4bb9V0q8lrZL0m1bJayC5oddI0mnAhcC5wA7ATsClwBEDmVcPc4AHgG2BfwK+L+nVA5uStYNWrm1J2wDXAzOArYGvAD+WNHpAExtgbug1kLQVcA5wQkTMjYg/R0RXRPw4Ij7byzTXSvpvSX+SdLukPXq8d6ik31a2OJ6W9JlKfDtJP5H0P5JekHSHpE3+7iTtCuwLnBURf4mI64CHgQ/WY/2tXK1e28BbgWURcW1ErI2IbwPPAh+ofe3blxt6bd4CDAd+0IdpbgQmAdsD9wPf6fHeFcAnI2JLYDJwSyV+OrAEeDXdW0pnAgEg6VJJl/ayrD2AJyJiVY/YQ5W42ca0em2r8m/D2OQ+5FuczQc6gTa3LfBcRKypdoKIuHL915LOBlZI2ioi/gR0AbtLeigiVgArKkO7gDHA+Ih4DLijx/w+tZHFjQT+tEHsT8DYavO1QavVa/tXwI6SpgHfB44BXgeMqDbfEnkLvTbPA9tJquo/RklDJJ0n6XFJK4FFlbe2q7x+EDgUWCzpNklvqcRnAI8BN0l6QtIZVeb3IjBqg9goYFVmrFlPLV3bEfE83fvyTwOWAYcAv6B7a3/QckOvzV3Ay8CRVY4/hu4ifDewFTChEhdARNwbEUfQ/ZH1h8A1lfiqiDg9IiYC7wdOk/SuKpa3AJgoacsesb0qcbONafXaJiJui4j9ImIb4FhgN+DXVeZbJDf0GlQ+Sn4RuETSkZJGSOqQ9F5JX8lMsiWwmu6tnxF0nz0AgKShkj5a+YjaBawE1lbeO0zSLpLUI762ivz+ADwInCVpuKSpwJ7AdbWst5Wv1Wu7Mu0+lZxGAecDSyLi5/1f6/bnhl6jiLiA7o99/0z3UfangBPp3grZ0FXAYuBp4LfA3Ru8fyywqPKR9R+Av6nEJ9H9cfJFurecLo2IXwJIulzS5RtJ8Wigk+59lucBH4qIZ/u2ljYYtUFtfw54rpLXGGBq39awPPIDLszMyuAtdDOzQrihm5kVwg3dzKwQbuhmZoWoqaFLOkTS7yU91oeLXcxanmvb2lG/z3KRNAT4A3Aw3Vdn3QtMi4jf9jbNUA2L4WzRr+WZbcrL/JlXYvWG9/foM9e2tZpqa7uWe7lMAR6LiCcAJF1N95VivRb9cLbgzdVdBGbWZ/fEvHrNyrVtLaXa2q5ll8tYuk/oX28JmZs+SZqu7hvkz+9idQ2LM2sa17a1pVoaem7zP9l/ExEzI6IzIjo7GFbD4syaxrVtbamWhr4EGNfj+9cCz9SWjllLcG1bW6qlod8LTJK0s6ShdN8z5Pr6pGU2oFzb1pb6fVA0ItZIOhH4OTAEuDIifFtWa3uubWtXNT2xKCJuAG6oUy5mLcO1be3IV4qamRXCDd3MrBBu6GZmhXBDNzMrhBu6mVkh3NDNzArhhm5mVgg3dDOzQrihm5kVwg3dzKwQbuhmZoVwQzczK4QbuplZIdzQzcwK4YZuZlYIN3Qzs0K4oZuZFcIN3cysEDU9gk7SImAVsBZYExGd9UiqFENGj87GX7x66yQ2b/L3q57viU8fmMSWfGT77Ng1Ty6uer72f1zb1o5qaugVfxURz9VhPmatxrVtbcW7XMzMClFrQw/gJkn3SZpej4TMWoRr29pOrbtcDoiIZyRtD9ws6XcRcXvPAZU/hukAwxlR4+LMmsa1bW2npi30iHim8roc+AEwJTNmZkR0RkRnB8NqWZxZ07i2rR31ewtd0hbAZhGxqvL1e4Bz6pZZAVbvOzEb/9kelyWxrqh+vhePvTOJfWT2Idmx8eH07Je1y5ZXv7BByLXdWpaf8NZsfLND0uPVR014IDt2qyEvJbFvnXVYduzIa+/pQ3atpZZdLjsAP5C0fj7fjYif1SUrs4Hl2ra21O+GHhFPAHvVMRezluDatnbl0xbNzArhhm5mVoh6XClqwLp37JPEPnHp3KYt/9sTf5qNf2xuerB05QfytwnwwVJrhM3Hj0tiS6amMYBrT52RxHbe/L7s2M1QTXldtuOQbHxkTXMdWN5CNzMrhBu6mVkh3NDNzArhhm5mVgg3dDOzQvgslzr50qyZSWxyRx+u52+QK8enFzi+7chPZ8e+5qZXJTE/IMP6IndGi2avSWL3T7q4lzmkNfi+378/O/Kpm8cnsWnTbsmO/cK2v01iq7fpJYU25i10M7NCuKGbmRXCDd3MrBBu6GZmhfBB0TrZZ2j6f2NXrG3Isna/7eNJ7KG3pwdle3PHv1yUjb9l+ClJ7DUX+aCoZey/ZzZ80H/+VxL77DaPJ7Ezl+2dnf7+49+YBh/6fXbsa9c8ncR++vY9smNzB0U7XswObWveQjczK4QbuplZIdzQzcwK4YZuZlaITTZ0SVdKWi7pkR6xbSTdLOnRyuvoxqZpVn+ubStNNWe5zAIuBq7qETsDmBcR50k6o/L95+ufXut56vuTs/EOPVjTfI9b9O4k9vwBK7JjJ5Iu68OvnZodO+WnTyaxM7d7ODv2pTenT0Yv3Cxc25u02YgRSextM+/Jjj1t9KNJ7ORn3pzEHv34Ltnp46EFVec1ZIf0QS17bvtMduzja/6SxMb95Nns2Macm9Ycm9xCj4jbgRc2CB8BzK58PRs4ss55mTWca9tK09996DtExFKAymv+mWZm7ce1bW2r4RcWSZoOTAcYTvrRzaxdubat1fR3C32ZpDEAlddeny4cETMjojMiOjsY1s/FmTWNa9vaVn+30K8HjgPOq7z+qG4ZtZDNttgiiY0a8XJ2bO4y/75c+r/4ot2S2Ejurnr6NUvSy6ABrp57UBI7+fj52bG52wdMZUrVORRiUNR2X/zloPRy+mlbfS07dlF663Mee+/WSWzdswurXv6QPdK/DYDHv5j+JzprzA+yYw+881NJbOeFD1WdQ7uo5rTFOcBdwG6Slkj6ON3FfrCkR4GDK9+btRXXtpVmk1voETGtl7feVedczJrKtW2l8ZWiZmaFcEM3MyuEG7qZWSH8gIuNePE96WX+t+z5H72MHlL1fD+2+JAkttWt6UMA6nEJ8k7/+qsk9vNjxmbHHr7FsiT2wt+/JTt2m2/eVVti1jYWH6YkttPm+fPu37XgA0lsxNbpduMfT5iUnf5V+z6fxObt883s2FGbDc/NITu2a+XQbLw03kI3MyuEG7qZWSHc0M3MCuGGbmZWCB8U3YgXXl/9gc6cG1/aLhtfeWQ637XP5u/N3Ajnzzg6Gz/87IuS2DVnz8iO/dgzpySxoT/P31LA2tvIJ6tvE/P2mJsGf1lrBrmDn3k3/yV/UHTiNetqTaIteAvdzKwQbuhmZoVwQzczK4QbuplZIXxQdCP+5e/m1DT9y9GRjTfzAGjODvPyD9I96fh3JrHLx92WHbt2uLcFBouxX78viR34dHp/cYAVR/65qnlufv+W2fgxx8xLYp/fNn/v9AVdrySx807+ZHbssHn3VpVXu/NfpZlZIdzQzcwK4YZuZlYIN3Qzs0JU80zRKyUtl/RIj9jZkp6W9GDl36GNTdOs/lzbVppqznKZBVwMXLVB/GsRcX7dM2ohHUrvSN6h/O0AcvF/X5De9xxgLAtqS6xGa55cnI3ftSS9/3vHTnfmZ5LeIrsdzWKQ1nZfxOrVSWzUd+/Ojh313erm2dt99icOW151Xh+ac2oS2/mGwX2f/k1uoUfE7cALTcjFrKlc21aaWvahnyjpN5WPraPrlpHZwHNtW1vqb0O/DHgdsDewFPhqbwMlTZc0X9L8LtKPbmYtxrVtbatfDT0ilkXE2ohYB3wDmLKRsTMjojMiOjsY1t88zZrCtW3trF+X/ksaExFLK99OBR7Z2Ph21RXpgc6uqP7RzTuNXpGN1+Phz40QkR7p7HV9o8HJDJDBUtvNtPnECUnsF/92QXbsSKX/MU76xfHZsZO+MLgPgOZssqFLmgMcBGwnaQlwFnCQpL3p/rNeBORvoGDWwlzbVppNNvSImJYJX9GAXMyayrVtpfGVomZmhXBDNzMrhBu6mVkh/ICLBvrojvdk41cxrsmZmDXHK3/dmcQ+etEPk1jubBaAve45Nom9/qTHsmNb9WyxgeQtdDOzQrihm5kVwg3dzKwQbuhmZoXwQdGNOH/G0Uns8LMvGoBMzFrLmne+KRuf/Y0Lk9jYISOS2AEPHZWdfqdPLE1ia1eu7GN2g5e30M3MCuGGbmZWCDd0M7NCuKGbmRXCDd3MrBA+y2Ujdpj3TBI76fh3ZsdePu62RqdTN9rvjdn4Zybf3ORMrB2sft9+Seykr30vOzZ3RsvxT70jiY0+I9961j7vZ3bXwlvoZmaFcEM3MyuEG7qZWSHc0M3MClHNQ6LHAVcBrwHWATMj4iJJ2wDfAybQ/TDdoyIi/5j7NrXmycVJ7NfXvTU7tuPUO5PYUSOXZ8de9rP0INGoaemPbu2K2n+c696xTxJ7/Kih2bF/O+rpJNahIfkZq6a0WsJgru2crvek9zIH+MrFlyaxvfIlxK63TE9iu528KImtW/G7PuVm1almC30NcHpEvAHYHzhB0u7AGcC8iJgEzKt8b9ZOXNtWlE029IhYGhH3V75eBSwExgJHALMrw2YDRzYqSbNGcG1bafq0D13SBGAf4B5gh4hYCt1/GMD2vUwzXdJ8SfO7WF1btmYN4tq2ElTd0CWNBK4DTomIqu9nGREzI6IzIjo7yD9H0GwgubatFFU1dEkddBf8dyJibiW8TNKYyvtjgPwRQLMW5tq2klRzlouAK4CFEXFBj7euB44Dzqu8/qghGbaYHWf8KhuftN0/JrFbp83Ijr1p8tVJ7I1f/2QS2+XY/IkVQ0aPTmKr952YHfuJS+cmscO3WJYd2xVp7Ig/vD87duRdi5JYuz2FfTDX9suHTUliX7hwdmYkvGloeqbTnncfmx27y7EPJLF2q4t2Vs29XA4AjgUelvRgJXYm3cV+jaSPA38EPtyYFM0axrVtRdlkQ4+IO+n9rON31Tcds+ZxbVtpfKWomVkh3NDNzArh+6HXycTP35XEDn7pc9mx930ifTL6OVOuT2JfvCC/63bsHulBzZ/tcdmmUuyX52aPz8ZHL0vX11qPOidn41O/fFMSe/vwVdmxu805OYnt+uXHs2N9AHRgeQvdzKwQbuhmZoVwQzczK4QbuplZIdzQzcwK4bNcGmj8ufOz8TfFKUnsgx+4I4k9fNTXs9PnHjqRu2y/r/a9Is1r/Kz8rQ6s9Ww+flwSO+vab2bHbqmuJDb5h6dmx076zN1JzGeztCZvoZuZFcIN3cysEG7oZmaFcEM3MyuED4o2UHS9ko3vdE56oPGBb6X3Mz9pzlbZ6S8fd1vVORz8yNFJ7MUbX5Mdu/Mv0/uvr6t6SdYs2jz/Zxuz0kOVew3Nz2P/cz+TxCZd4gPg7c5b6GZmhXBDNzMrhBu6mVkh3NDNzAqxyYYuaZykWyUtlLRA0qcr8bMlPS3pwcq/Qxufrln9uLatNNWc5bIGOD0i7pe0JXCfpJsr730tIs5vXHqDx5onFyexJfvnxx7Gm6qe70ieqCoGg/KMlras7cfP3S8b/92ulySxXW+Znh27i89oKVI1D4leCiytfL1K0kJgbKMTM2s017aVpk/70CVNAPYB7qmETpT0G0lXShrdyzTTJc2XNL+L1TUla9Yorm0rQdUNXdJI4DrglIhYCVwGvA7Ym+6tnK/mpouImRHRGRGdHQyrQ8pm9eXatlJU1dAlddBd8N+JiLkAEbEsItZGxDrgG8CUxqVp1hiubSvJJvehSxJwBbAwIi7oER9T2QcJMBV4pDEpmjVGu9b2xM/dlY0f+rl9k9guPNDodKyFVHOWywHAscDDkh6sxM4EpknaGwhgEfDJhmRo1jiubStKNWe53Ako89YN9U/HrHlc21YaXylqZlYIN3Qzs0K4oZuZFcIN3cysEG7oZmaFcEM3MyuEG7qZWSHc0M3MCqGIaN7CpGeB9Tf+3g54rmkLbx6v18AZHxGvHogF96jtdvg59Vep69YO61VVbTe1of+/BUvzI6JzQBbeQF6vwa3kn1Op61bSenmXi5lZIdzQzcwKMZANfeYALruRvF6DW8k/p1LXrZj1GrB96GZmVl/e5WJmVoimN3RJh0j6vaTHJJ3R7OXXU+UBwsslPdIjto2kmyU9WnnNPmC4lUkaJ+lWSQslLZD06Uq87detkUqpbdd1+63bek1t6JKGAJcA7wV2p/vJMLs3M4c6mwUcskHsDGBeREwC5lW+bzdrgNMj4g3A/sAJld9TCevWEIXV9ixc122p2VvoU4DHIuKJiHgFuBo4osk51E1E3A68sEH4CGB25evZwJFNTaoOImJpRNxf+XoVsBAYSwHr1kDF1Lbruv3Wbb1mN/SxwFM9vl9SiZVkh/UPGK68bj/A+dRE0gRgH+AeClu3Oiu9tov63Zda181u6LnnN/o0mxYlaSRwHXBKRKwc6HxanGu7TZRc181u6EuAcT2+fy3wTJNzaLRlksYAVF6XD3A+/SKpg+6i/05EzK2Ei1i3Bim9tov43Zde181u6PcCkyTtLGkocDRwfZNzaLTrgeMqXx8H/GgAc+kXSQKuABZGxAU93mr7dWug0mu77X/3g6Gum35hkaRDgQuBIcCVEfGlpiZQR5LmAAfRfbe2ZcBZwA+Ba4CdgD8CH46IDQ8wtTRJBwJ3AA8D6yrhM+ne39jW69ZIpdS267r91m09XylqZlYIXylqZlYIN3Qzs0K4oZuZFcIN3cysEG7oZmaFcEM3MyuEG7qZWSHc0M3MCvG/YlW38+c/T+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 582\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(torch.argmax(train_target[sample])))\n",
    "axis[1].imshow(train_input[sample+1,0])\n",
    "axis[1].set_title('Class: {}'.format(torch.argmax(train_target[sample+1])))\n",
    "plt.show()\n",
    "print('image size:',train_input[sample,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor example\n",
    "class ProfessorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProfessorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from professor's example, without maxPooling\n",
    "class SimplifiedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64*20*20, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*20*20)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    criterion = nn.MSELoss()\n",
    "    eta = 1e-1\n",
    "    for e in range(0, 25):\n",
    "        sum_loss = 0\n",
    "        # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def compute_nb_errors(model, input_, target, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, input_.size(0), mini_batch_size):\n",
    "        output = model(input_.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = target[b:b+mini_batch_size].argmax(1)\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/input_.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8870525732636452\n",
      "1 0.7685782760381699\n",
      "2 0.6961821764707565\n",
      "3 0.6333322450518608\n",
      "4 0.5808088779449463\n",
      "5 0.5388898029923439\n",
      "6 0.5061322301626205\n",
      "7 0.4807812049984932\n",
      "8 0.45905613899230957\n",
      "9 0.4392935335636139\n",
      "10 0.42149854078888893\n",
      "11 0.404970396310091\n",
      "12 0.3899703547358513\n",
      "13 0.39278144761919975\n",
      "14 0.3700929917395115\n",
      "15 0.3485518004745245\n",
      "16 0.3407313097268343\n",
      "17 0.3286995254456997\n",
      "18 0.31756569631397724\n",
      "19 0.3365845810621977\n",
      "20 0.3131201360374689\n",
      "21 0.2894383352249861\n",
      "22 0.2835408467799425\n",
      "23 0.2781979199498892\n",
      "24 0.2740096766501665\n",
      "Loss on test set: 15.7 %\n"
     ]
    }
   ],
   "source": [
    "# Train the professor's model and evaluate it\n",
    "model = ProfessorNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8254552185535431\n",
      "1 0.6248150393366814\n",
      "2 0.5219256095588207\n",
      "3 0.41258498281240463\n",
      "4 0.3728884756565094\n",
      "5 0.34388227574527264\n",
      "6 0.3034926690161228\n",
      "7 0.2802353519946337\n",
      "8 0.25411636382341385\n",
      "9 0.24268661998212337\n",
      "10 0.23081058077514172\n",
      "11 0.20639300346374512\n",
      "12 0.19570204615592957\n",
      "13 0.17976822145283222\n",
      "14 0.179344835691154\n",
      "15 0.16625335346907377\n",
      "16 0.1509782550856471\n",
      "17 0.1493746917694807\n",
      "18 0.1418860461562872\n",
      "19 0.1328622940927744\n",
      "20 0.12497582007199526\n",
      "21 0.12860154546797276\n",
      "22 0.12318934593349695\n",
      "23 0.11285346280783415\n",
      "24 0.10832641366869211\n",
      "Loss on test set: 8.799999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Train the simplified model and evaluate it\n",
    "model = SimplifiedNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "nb = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 2, 14, 14])\n",
      "train_target: torch.Size([1000])\n",
      "train_classes: torch.Size([1000, 2])\n",
      "test_input: torch.Size([1000, 2, 14, 14])\n",
      "test_target: torch.Size([1000])\n",
      "test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Display the data shape\n",
    "print('train_input:',train_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('train_classes:',train_classes.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('test_target:',test_target.shape)\n",
    "print('test_classes:',test_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected classification:  0\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 593\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(train_classes[sample,0]))\n",
    "axis[1].imshow(train_input[sample,1])\n",
    "axis[1].set_title('Class: {}'.format(train_classes[sample,1]))\n",
    "plt.show()\n",
    "print('Expected classification: ',train_target[sample].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Define a module responsible for recognizing the classes and another that will identify if the first digit is less or equal to the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Net that works with a single image (hence channel dim = 1)\n",
    "class Parallel_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to classify the digit.\n",
    "\n",
    "        Net input: Nx1x14x14 (single image)\n",
    "        Net output: 10x1\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Parallel_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*3*3, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*3*3)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to infer digit's relation.\n",
    "\n",
    "        Net input: 20x1 (digit classification)\n",
    "        Net output: 1x1 (bigger or smaller)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Analyzer_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2*10, 15)\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.fc4 = nn.Linear(5, 2) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)''' # torch.sigmoid() maybe?\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Basic Network with only a MSE loss on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    '''\n",
    "        Net caracteristics:\n",
    "            - No weight sharing\n",
    "            - No intermediate loss\n",
    "        \n",
    "        Net input: Nx2x14x14\n",
    "        Net output: 2x1 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.parallel_net1 = Parallel_Net()\n",
    "        self.parallel_net2 = Parallel_Net()\n",
    "        self.analyser_net  = Analyzer_Net()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Split the 2 input channels\n",
    "        x1 = x[:,0,:,:].view(-1,1,14,14)\n",
    "        x2 = x[:,1,:,:].view(-1,1,14,14)\n",
    "\n",
    "        # No weight sharing (declare 2 distinct instances of Parallel_Net)\n",
    "        x1 = self.parallel_net1(x1)\n",
    "        x2 = self.parallel_net2(x2)\n",
    "\n",
    "        # Concatenate back both classification results \n",
    "        x = torch.cat((x1.view(-1,10),x2.view(-1,10)),dim=1)\n",
    "        x = self.analyser_net(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMSE(model, train_input, train_target, mini_batch_size):\n",
    "    train_target = train_target.type(torch.FloatTensor)\n",
    "\n",
    "    # Specify the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.01\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFinalOutput(model,test_input, test_target,mini_batch_size):\n",
    "    test_target = test_target.type(torch.FloatTensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        error = 0\n",
    "        for b in range(0, test_input.size(0), mini_batch_size):\n",
    "            output = model(test_input.narrow(0, b, mini_batch_size))\n",
    "            for i in range(output.size(0)):\n",
    "                #print(output[i].item(),test_target.narrow(0, b, batch_size)[i].item())\n",
    "                if output[i].item() >= 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() < 0.2:\n",
    "                        error += 1\n",
    "                elif output[i].item() < 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() > 0.8:\n",
    "                        error += 1\n",
    "                else:\n",
    "                    error += 1\n",
    "    return error/test_target.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 99352\n"
     ]
    }
   ],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Create an instance of the network\n",
    "basicModel = BasicNet()\n",
    "num_param = sum(p.numel() for p in basicModel.parameters() if p.requires_grad)\n",
    "print('Number of trainable parameters:',num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 1.4364519715309143\n",
      "Sum of loss at epoch 1: \t 1.2671064138412476\n",
      "Sum of loss at epoch 2: \t 1.2653934359550476\n",
      "Sum of loss at epoch 3: \t 1.2639169991016388\n",
      "Sum of loss at epoch 4: \t 1.2626781463623047\n",
      "Sum of loss at epoch 5: \t 1.2615963518619537\n",
      "Sum of loss at epoch 6: \t 1.2606699168682098\n",
      "Sum of loss at epoch 7: \t 1.2598430216312408\n",
      "Sum of loss at epoch 8: \t 1.259132742881775\n",
      "Sum of loss at epoch 9: \t 1.25843845307827\n",
      "Sum of loss at epoch 10: \t 1.257744938135147\n",
      "Sum of loss at epoch 11: \t 1.2571662068367004\n",
      "Sum of loss at epoch 12: \t 1.2567053139209747\n",
      "Sum of loss at epoch 13: \t 1.256271868944168\n",
      "Sum of loss at epoch 14: \t 1.2558508962392807\n",
      "Sum of loss at epoch 15: \t 1.2554482817649841\n",
      "Sum of loss at epoch 16: \t 1.2550633698701859\n",
      "Sum of loss at epoch 17: \t 1.254713088274002\n",
      "Sum of loss at epoch 18: \t 1.2543928176164627\n",
      "Sum of loss at epoch 19: \t 1.2540881782770157\n",
      "Sum of loss at epoch 20: \t 1.2537960559129715\n",
      "Sum of loss at epoch 21: \t 1.2535055577754974\n",
      "Sum of loss at epoch 22: \t 1.2532324492931366\n",
      "Sum of loss at epoch 23: \t 1.2529815584421158\n",
      "Sum of loss at epoch 24: \t 1.2527200877666473\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "basicModel = trainMSE(basicModel,train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  50.1 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "res = evaluateFinalOutput(basicModel,test_input,test_target,mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cross-entropy loss at the classification level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Distinction between `clone()` and `copy_()`\n",
    "Both functions allow to create a clone of a Tensor instance. The difference is that using `clone()`, this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor!\n",
    "Therefore `copy_()` allows to fully separate the cloned tensor from its original version and makes it independent.\n",
    "\n",
    "Check [the doc](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone) for more details (`copy_` is right below `clone`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train each submodule individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Class Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassIdentifier(model, train_input, train_classes, mini_batch_size):\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.01\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            for i in range(2):\n",
    "                # For each image (since there are 2 channels)\n",
    "                output = model(train_input[:,i].view(-1,1,14,14).narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "                loss = criterion(output, train_classes[:,i].narrow(0, b, mini_batch_size))\n",
    "                sum_loss += loss.item()\n",
    "                loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def evaluateClassIdentification(model, test_input, test_classes, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input[:,0].view(-1,1,14,14).narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = test_classes[:,0][b:b+mini_batch_size]\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/test_input.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Instantiate the model\n",
    "classModel = Parallel_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 60.63195204734802\n",
      "Sum of loss at epoch 1: \t 22.20393466949463\n",
      "Sum of loss at epoch 2: \t 20.315518379211426\n",
      "Sum of loss at epoch 3: \t 19.85521388053894\n",
      "Sum of loss at epoch 4: \t 18.008901953697205\n",
      "Sum of loss at epoch 5: \t 12.561357140541077\n",
      "Sum of loss at epoch 6: \t 13.687849044799805\n",
      "Sum of loss at epoch 7: \t 10.33372688293457\n",
      "Sum of loss at epoch 8: \t 7.455751061439514\n",
      "Sum of loss at epoch 9: \t 6.146078497171402\n",
      "Sum of loss at epoch 10: \t 6.269042432308197\n",
      "Sum of loss at epoch 11: \t 4.6116723120212555\n",
      "Sum of loss at epoch 12: \t 5.821642756462097\n",
      "Sum of loss at epoch 13: \t 3.4021140038967133\n",
      "Sum of loss at epoch 14: \t 2.979199454188347\n",
      "Sum of loss at epoch 15: \t 4.020108565688133\n",
      "Sum of loss at epoch 16: \t 2.457531914114952\n",
      "Sum of loss at epoch 17: \t 3.018318757414818\n",
      "Sum of loss at epoch 18: \t 3.0352158695459366\n",
      "Sum of loss at epoch 19: \t 1.943548545241356\n",
      "Sum of loss at epoch 20: \t 1.7361843436956406\n",
      "Sum of loss at epoch 21: \t 1.597754642367363\n",
      "Sum of loss at epoch 22: \t 1.5227702409029007\n",
      "Sum of loss at epoch 23: \t 1.399191565811634\n",
      "Sum of loss at epoch 24: \t 1.5717825889587402\n"
     ]
    }
   ],
   "source": [
    "classModel = trainClassIdentifier(classModel, train_input, train_classes, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  7.199999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the class identification of the model\n",
    "res = evaluateClassIdentification(classModel, test_input, test_classes, mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAnalyzer(model, train_classes, train_target, mini_batch_size):\n",
    "    #train_target = train_target.type(torch.FloatTensor)\n",
    "    # Classify into one-hot\n",
    "    train_target_oneHot = torch.eye(2)[train_target]\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 1e-0\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n",
    "    \n",
    "    # One hot encode the training classes and concatenate them (1000x2)->(1000x2x10)\n",
    "    train_oneHot = torch.empty([train_classes.shape[0],2,10])\n",
    "    train_oneHot[:,0] = torch.eye(10)[train_classes[:,0]]\n",
    "    train_oneHot[:,1] = torch.eye(10)[train_classes[:,1]]\n",
    "    # Convert to the format expected by the Analyzer (1000x20)\n",
    "    train_oneHot_cat = torch.cat([train_oneHot[:,0],train_oneHot[:,1]],dim=1)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_oneHot_cat.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model(train_oneHot_cat.narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "            loss = criterion(output, train_target_oneHot.narrow(0, b, mini_batch_size))\n",
    "            sum_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            '''for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)'''\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([9, 5, 7, 9, 8, 7, 8, 4, 6, 9])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1.]])\n",
      "tensor([9, 5, 7, 9, 8, 7, 8, 4, 6, 9]) tensor([3, 4, 4, 6, 8, 4, 9, 0, 8, 9])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1])\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Verifiy One Hot encoding and concatenation\n",
    "train_oneHot = torch.empty([train_classes.shape[0],2,10])\n",
    "train_oneHot[:,0] = torch.eye(10)[train_classes[:,0]]\n",
    "train_oneHot[:,1] = torch.eye(10)[train_classes[:,1]]\n",
    "train_oh_cat = torch.cat([train_oneHot[:,0],train_oneHot[:,1]],dim=1)\n",
    "print(train_oneHot[0:10,0])\n",
    "print(train_classes[0:10,0])\n",
    "print(train_oh_cat[0:10])\n",
    "print(train_classes[0:10,0],train_classes[0:10,1])\n",
    "print(train_target[0:10])\n",
    "\n",
    "train_target_binary = torch.eye(2)[train_target]\n",
    "print(train_target_binary[0:10])\n",
    "print(train_target[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAnalyzer(model,test_classes, test_target,mini_batch_size):\n",
    "    #test_target = test_target.type(torch.FloatTensor)\n",
    "    train_target_oneHot = torch.eye(2)[train_target]\n",
    "    \n",
    "    # One hot encode the training classes and concatenate them (1000x2)->(1000x2x10)\n",
    "    test_oneHot = torch.empty([test_classes.shape[0],2,10])\n",
    "    test_oneHot[:,0] = torch.eye(10)[test_classes[:,0]]\n",
    "    test_oneHot[:,1] = torch.eye(10)[test_classes[:,1]]\n",
    "    # Convert to the format expected by the Analyzer (1000x20)\n",
    "    test_oneHot_cat = torch.cat([test_oneHot[:,0],test_oneHot[:,1]],dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        error = 0\n",
    "        for b in range(0, test_oneHot_cat.size(0), mini_batch_size):\n",
    "            output = model(test_oneHot_cat.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            '''samp = 143\n",
    "            print(output[samp].item())\n",
    "            print(test_target[b+samp].item())\n",
    "            print(test_oneHot_cat.narrow(0, b, mini_batch_size)[samp])\n",
    "            return'''\n",
    "            \n",
    "            for i in range(output.size(0)):\n",
    "                if torch.argmax(output[i]).item() >= 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() < 0.2:\n",
    "                        error += 1\n",
    "                elif torch.argmax(output[i]).item() < 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() > 0.8:\n",
    "                        error += 1\n",
    "                else:\n",
    "                    error += 1\n",
    "    return error/test_target.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Instantiate the model\n",
    "analyzerModel = Analyzer_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 1.2639853060245514\n",
      "Sum of loss at epoch 1: \t 1.2460277378559113\n",
      "Sum of loss at epoch 2: \t 1.2381724119186401\n",
      "Sum of loss at epoch 3: \t 1.2335583567619324\n",
      "Sum of loss at epoch 4: \t 1.2296762764453888\n",
      "Sum of loss at epoch 5: \t 1.2253766357898712\n",
      "Sum of loss at epoch 6: \t 1.2197979241609573\n",
      "Sum of loss at epoch 7: \t 1.2118326872587204\n",
      "Sum of loss at epoch 8: \t 1.1995779275894165\n",
      "Sum of loss at epoch 9: \t 1.1793096959590912\n",
      "Sum of loss at epoch 10: \t 1.1431872993707657\n",
      "Sum of loss at epoch 11: \t 1.0745759159326553\n",
      "Sum of loss at epoch 12: \t 0.9451175630092621\n",
      "Sum of loss at epoch 13: \t 0.7439098805189133\n",
      "Sum of loss at epoch 14: \t 0.533257432281971\n",
      "Sum of loss at epoch 15: \t 0.39170412719249725\n",
      "Sum of loss at epoch 16: \t 0.30550817027688026\n",
      "Sum of loss at epoch 17: \t 0.21205883845686913\n",
      "Sum of loss at epoch 18: \t 0.3185892552137375\n",
      "Sum of loss at epoch 19: \t 0.32614798843860626\n",
      "Sum of loss at epoch 20: \t 0.30627647414803505\n",
      "Sum of loss at epoch 21: \t 0.1335315015166998\n",
      "Sum of loss at epoch 22: \t 0.10741676203906536\n",
      "Sum of loss at epoch 23: \t 0.2247218918055296\n",
      "Sum of loss at epoch 24: \t 0.26403797790408134\n"
     ]
    }
   ],
   "source": [
    "analyzerModel = trainAnalyzer(analyzerModel, train_classes, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  11.1 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the relation prediction between the digit of the model\n",
    "res = evaluateAnalyzer(analyzerModel,test_classes,test_target,mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
