{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with the effect of maxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import dlc_practical_prologue as prologue\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset (reduced set)\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 1, 28, 28])\n",
      "test_input: torch.Size([1000, 1, 28, 28])\n",
      "train_target: torch.Size([1000, 10])\n",
      "test_target: torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "# Visualize dataset dimensions\n",
    "print('train_input:',train_input.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('test_target:',test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 582\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(torch.argmax(train_target[sample])))\n",
    "axis[1].imshow(train_input[sample+1,0])\n",
    "axis[1].set_title('Class: {}'.format(torch.argmax(train_target[sample+1])))\n",
    "plt.show()\n",
    "print('image size:',train_input[sample,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor example\n",
    "class ProfessorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProfessorNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from professor's example, without maxPooling\n",
    "class SimplifiedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64*20*20, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*20*20)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_model(model, train_input, train_target, mini_batch_size):\n",
    "    criterion = nn.MSELoss()\n",
    "    eta = 1e-1\n",
    "    for e in range(0, 25):\n",
    "        sum_loss = 0\n",
    "        # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def compute_nb_errors(model, input_, target, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, input_.size(0), mini_batch_size):\n",
    "        output = model(input_.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = target[b:b+mini_batch_size].argmax(1)\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/input_.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8870525732636452\n",
      "1 0.7685782760381699\n",
      "2 0.6961821764707565\n",
      "3 0.6333322450518608\n",
      "4 0.5808088779449463\n",
      "5 0.5388898029923439\n",
      "6 0.5061322301626205\n",
      "7 0.4807812049984932\n",
      "8 0.45905613899230957\n",
      "9 0.4392935335636139\n",
      "10 0.42149854078888893\n",
      "11 0.404970396310091\n",
      "12 0.3899703547358513\n",
      "13 0.39278144761919975\n",
      "14 0.3700929917395115\n",
      "15 0.3485518004745245\n",
      "16 0.3407313097268343\n",
      "17 0.3286995254456997\n",
      "18 0.31756569631397724\n",
      "19 0.3365845810621977\n",
      "20 0.3131201360374689\n",
      "21 0.2894383352249861\n",
      "22 0.2835408467799425\n",
      "23 0.2781979199498892\n",
      "24 0.2740096766501665\n",
      "Loss on test set: 15.7 %\n"
     ]
    }
   ],
   "source": [
    "# Train the professor's model and evaluate it\n",
    "model = ProfessorNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8254552185535431\n",
      "1 0.6248150393366814\n",
      "2 0.5219256095588207\n",
      "3 0.41258498281240463\n",
      "4 0.3728884756565094\n",
      "5 0.34388227574527264\n",
      "6 0.3034926690161228\n",
      "7 0.2802353519946337\n",
      "8 0.25411636382341385\n",
      "9 0.24268661998212337\n",
      "10 0.23081058077514172\n",
      "11 0.20639300346374512\n",
      "12 0.19570204615592957\n",
      "13 0.17976822145283222\n",
      "14 0.179344835691154\n",
      "15 0.16625335346907377\n",
      "16 0.1509782550856471\n",
      "17 0.1493746917694807\n",
      "18 0.1418860461562872\n",
      "19 0.1328622940927744\n",
      "20 0.12497582007199526\n",
      "21 0.12860154546797276\n",
      "22 0.12318934593349695\n",
      "23 0.11285346280783415\n",
      "24 0.10832641366869211\n",
      "Loss on test set: 8.799999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Train the simplified model and evaluate it\n",
    "model = SimplifiedNet() \n",
    "mini_batch_size = 100\n",
    "\n",
    "train_model(model, train_input, train_target, mini_batch_size)\n",
    "print('Loss on test set:', compute_nb_errors(model, test_input, test_target, mini_batch_size),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "nb = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input: torch.Size([1000, 2, 14, 14])\n",
      "train_target: torch.Size([1000])\n",
      "train_classes: torch.Size([1000, 2])\n",
      "test_input: torch.Size([1000, 2, 14, 14])\n",
      "test_target: torch.Size([1000])\n",
      "test_classes: torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Display the data shape\n",
    "print('train_input:',train_input.shape)\n",
    "print('train_target:',train_target.shape)\n",
    "print('train_classes:',train_classes.shape)\n",
    "print('test_input:',test_input.shape)\n",
    "print('test_target:',test_target.shape)\n",
    "print('test_classes:',test_classes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected classification:  0\n"
     ]
    }
   ],
   "source": [
    "# Dataset visualization\n",
    "sample = 593\n",
    "fig,axis = plt.subplots(1,2)\n",
    "axis[0].imshow(train_input[sample,0])\n",
    "axis[0].set_title('Class: {}'.format(train_classes[sample,0]))\n",
    "axis[1].imshow(train_input[sample,1])\n",
    "axis[1].set_title('Class: {}'.format(train_classes[sample,1]))\n",
    "plt.show()\n",
    "print('Expected classification: ',train_target[sample].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "Define a module responsible for recognizing the classes and another that will identify if the first digit is less or equal to the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Net that works with a single image (hence channel dim = 1)\n",
    "class Parallel_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to classify the digit.\n",
    "\n",
    "        Net input: Nx1x14x14 (single image)\n",
    "        Net output: 10x1\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Parallel_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*3*3, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 64*3*3)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer_Net(nn.Module):\n",
    "    '''\n",
    "        Building block used to infer digit's relation.\n",
    "\n",
    "        Net input: 20x1 (digit classification)\n",
    "        Net output: 1x1 (bigger or smaller)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Analyzer_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2*10, 15)\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.fc4 = nn.Linear(5, 1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Basic Network with only a MSE loss on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    '''\n",
    "        Net caracteristics:\n",
    "            - No weight sharing\n",
    "            - No intermediate loss\n",
    "        \n",
    "        Net input: Nx2x14x14\n",
    "        Net output: 2x1 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.parallel_net1 = Parallel_Net()\n",
    "        self.parallel_net2 = Parallel_Net()\n",
    "        self.analyser_net  = Analyzer_Net()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Split the 2 input channels\n",
    "        x1 = x[:,0,:,:].view(-1,1,14,14)\n",
    "        x2 = x[:,1,:,:].view(-1,1,14,14)\n",
    "\n",
    "        # No weight sharing (declare 2 distinct instances of Parallel_Net)\n",
    "        x1 = self.parallel_net1(x1)\n",
    "        x2 = self.parallel_net2(x2)\n",
    "\n",
    "        # Concatenate back both classification results \n",
    "        x = torch.cat((x1.view(-1,10),x2.view(-1,10)),dim=1)\n",
    "        x = self.analyser_net(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMSE(model, train_input, train_target, mini_batch_size):\n",
    "    train_target = train_target.type(torch.FloatTensor)\n",
    "\n",
    "    # Specify the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 250\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.1\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFinalOutput(model,test_input, test_target,mini_batch_size):\n",
    "    test_target = test_target.type(torch.FloatTensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        error = 0\n",
    "        for b in range(0, test_input.size(0), mini_batch_size):\n",
    "            output = model(test_input.narrow(0, b, mini_batch_size))\n",
    "            for i in range(output.size(0)):\n",
    "                #print(output[i].item(),test_target.narrow(0, b, batch_size)[i].item())\n",
    "                if output[i].item() >= 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() < 0.2:\n",
    "                        error += 1\n",
    "                elif output[i].item() < 0.5:\n",
    "                    if test_target.narrow(0, b, mini_batch_size)[i].item() > 0.8:\n",
    "                        error += 1\n",
    "                else:\n",
    "                    error += 1\n",
    "    return error/test_target.size(0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 99352\n"
     ]
    }
   ],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "# Create an instance of the network\n",
    "basicModel = BasicNet()\n",
    "num_param = sum(p.numel() for p in basicModel.parameters() if p.requires_grad)\n",
    "print('Number of trainable parameters:',num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 1.3709479868412018\n",
      "Sum of loss at epoch 1: \t 1.246729537844658\n",
      "Sum of loss at epoch 2: \t 1.2430558055639267\n",
      "Sum of loss at epoch 3: \t 1.2428212314844131\n",
      "Sum of loss at epoch 4: \t 1.2427321374416351\n",
      "Sum of loss at epoch 5: \t 1.242651715874672\n",
      "Sum of loss at epoch 6: \t 1.242592602968216\n",
      "Sum of loss at epoch 7: \t 1.2425462752580643\n",
      "Sum of loss at epoch 8: \t 1.2424951493740082\n",
      "Sum of loss at epoch 9: \t 1.2424569576978683\n",
      "Sum of loss at epoch 10: \t 1.2424254566431046\n",
      "Sum of loss at epoch 11: \t 1.2424008250236511\n",
      "Sum of loss at epoch 12: \t 1.242388278245926\n",
      "Sum of loss at epoch 13: \t 1.242376908659935\n",
      "Sum of loss at epoch 14: \t 1.2423651665449142\n",
      "Sum of loss at epoch 15: \t 1.2423565834760666\n",
      "Sum of loss at epoch 16: \t 1.2423472851514816\n",
      "Sum of loss at epoch 17: \t 1.242339938879013\n",
      "Sum of loss at epoch 18: \t 1.2423331141471863\n",
      "Sum of loss at epoch 19: \t 1.242326557636261\n",
      "Sum of loss at epoch 20: \t 1.2423202842473984\n",
      "Sum of loss at epoch 21: \t 1.2423145622015\n",
      "Sum of loss at epoch 22: \t 1.2423085719347\n",
      "Sum of loss at epoch 23: \t 1.2423024624586105\n",
      "Sum of loss at epoch 24: \t 1.242296427488327\n",
      "Sum of loss at epoch 25: \t 1.2422905415296555\n",
      "Sum of loss at epoch 26: \t 1.242286205291748\n",
      "Sum of loss at epoch 27: \t 1.2422807812690735\n",
      "Sum of loss at epoch 28: \t 1.242275983095169\n",
      "Sum of loss at epoch 29: \t 1.2422704249620438\n",
      "Sum of loss at epoch 30: \t 1.2422660440206528\n",
      "Sum of loss at epoch 31: \t 1.242261290550232\n",
      "Sum of loss at epoch 32: \t 1.2422555685043335\n",
      "Sum of loss at epoch 33: \t 1.2422500997781754\n",
      "Sum of loss at epoch 34: \t 1.2422457039356232\n",
      "Sum of loss at epoch 35: \t 1.2422440350055695\n",
      "Sum of loss at epoch 36: \t 1.2422368228435516\n",
      "Sum of loss at epoch 37: \t 1.2422322928905487\n",
      "Sum of loss at epoch 38: \t 1.2422271519899368\n",
      "Sum of loss at epoch 39: \t 1.2422228455543518\n",
      "Sum of loss at epoch 40: \t 1.2422189861536026\n",
      "Sum of loss at epoch 41: \t 1.242215022444725\n",
      "Sum of loss at epoch 42: \t 1.242210403084755\n",
      "Sum of loss at epoch 43: \t 1.2422063499689102\n",
      "Sum of loss at epoch 44: \t 1.2422023117542267\n",
      "Sum of loss at epoch 45: \t 1.242197498679161\n",
      "Sum of loss at epoch 46: \t 1.2421944588422775\n",
      "Sum of loss at epoch 47: \t 1.2421892136335373\n",
      "Sum of loss at epoch 48: \t 1.2421855628490448\n",
      "Sum of loss at epoch 49: \t 1.2421820163726807\n",
      "Sum of loss at epoch 50: \t 1.2421781420707703\n",
      "Sum of loss at epoch 51: \t 1.2421760559082031\n",
      "Sum of loss at epoch 52: \t 1.242173358798027\n",
      "Sum of loss at epoch 53: \t 1.242168515920639\n",
      "Sum of loss at epoch 54: \t 1.2421672493219376\n",
      "Sum of loss at epoch 55: \t 1.242162600159645\n",
      "Sum of loss at epoch 56: \t 1.2421603053808212\n",
      "Sum of loss at epoch 57: \t 1.242156058549881\n",
      "Sum of loss at epoch 58: \t 1.242152526974678\n",
      "Sum of loss at epoch 59: \t 1.2421475797891617\n",
      "Sum of loss at epoch 60: \t 1.2421413958072662\n",
      "Sum of loss at epoch 61: \t 1.2421379834413528\n",
      "Sum of loss at epoch 62: \t 1.2421341985464096\n",
      "Sum of loss at epoch 63: \t 1.2421297878026962\n",
      "Sum of loss at epoch 64: \t 1.2421274483203888\n",
      "Sum of loss at epoch 65: \t 1.2421270161867142\n",
      "Sum of loss at epoch 66: \t 1.2421229481697083\n",
      "Sum of loss at epoch 67: \t 1.2421188950538635\n",
      "Sum of loss at epoch 68: \t 1.2421145290136337\n",
      "Sum of loss at epoch 69: \t 1.2421118021011353\n",
      "Sum of loss at epoch 70: \t 1.2421070486307144\n",
      "Sum of loss at epoch 71: \t 1.2421035021543503\n",
      "Sum of loss at epoch 72: \t 1.242099568247795\n",
      "Sum of loss at epoch 73: \t 1.2420959621667862\n",
      "Sum of loss at epoch 74: \t 1.2420924454927444\n",
      "Sum of loss at epoch 75: \t 1.2420899122953415\n",
      "Sum of loss at epoch 76: \t 1.2420884519815445\n",
      "Sum of loss at epoch 77: \t 1.2420860081911087\n",
      "Sum of loss at epoch 78: \t 1.2420848608016968\n",
      "Sum of loss at epoch 79: \t 1.2420813888311386\n",
      "Sum of loss at epoch 80: \t 1.2420768737792969\n",
      "Sum of loss at epoch 81: \t 1.2420737743377686\n",
      "Sum of loss at epoch 82: \t 1.2420708686113358\n",
      "Sum of loss at epoch 83: \t 1.2420659959316254\n",
      "Sum of loss at epoch 84: \t 1.2420632243156433\n",
      "Sum of loss at epoch 85: \t 1.2420592457056046\n",
      "Sum of loss at epoch 86: \t 1.242056667804718\n",
      "Sum of loss at epoch 87: \t 1.2420532256364822\n",
      "Sum of loss at epoch 88: \t 1.2420501857995987\n",
      "Sum of loss at epoch 89: \t 1.242047056555748\n",
      "Sum of loss at epoch 90: \t 1.2420480251312256\n",
      "Sum of loss at epoch 91: \t 1.2420436292886734\n",
      "Sum of loss at epoch 92: \t 1.242044821381569\n",
      "Sum of loss at epoch 93: \t 1.2420369982719421\n",
      "Sum of loss at epoch 94: \t 1.24203260242939\n",
      "Sum of loss at epoch 95: \t 1.2420291602611542\n",
      "Sum of loss at epoch 96: \t 1.2420304715633392\n",
      "Sum of loss at epoch 97: \t 1.2420298606157303\n",
      "Sum of loss at epoch 98: \t 1.2420240193605423\n",
      "Sum of loss at epoch 99: \t 1.2420229762792587\n",
      "Sum of loss at epoch 100: \t 1.2420168668031693\n",
      "Sum of loss at epoch 101: \t 1.2420136630535126\n",
      "Sum of loss at epoch 102: \t 1.2420112639665604\n",
      "Sum of loss at epoch 103: \t 1.242007538676262\n",
      "Sum of loss at epoch 104: \t 1.242004781961441\n",
      "Sum of loss at epoch 105: \t 1.2420011311769485\n",
      "Sum of loss at epoch 106: \t 1.2419977337121964\n",
      "Sum of loss at epoch 107: \t 1.2419971376657486\n",
      "Sum of loss at epoch 108: \t 1.2419962286949158\n",
      "Sum of loss at epoch 109: \t 1.2419921457767487\n",
      "Sum of loss at epoch 110: \t 1.2419872730970383\n",
      "Sum of loss at epoch 111: \t 1.2419835925102234\n",
      "Sum of loss at epoch 112: \t 1.2419794797897339\n",
      "Sum of loss at epoch 113: \t 1.2419771403074265\n",
      "Sum of loss at epoch 114: \t 1.241971716284752\n",
      "Sum of loss at epoch 115: \t 1.2419707924127579\n",
      "Sum of loss at epoch 116: \t 1.2419683188199997\n",
      "Sum of loss at epoch 117: \t 1.2419635653495789\n",
      "Sum of loss at epoch 118: \t 1.2419603019952774\n",
      "Sum of loss at epoch 119: \t 1.2419583052396774\n",
      "Sum of loss at epoch 120: \t 1.2419551610946655\n",
      "Sum of loss at epoch 121: \t 1.241951271891594\n",
      "Sum of loss at epoch 122: \t 1.2419456988573074\n",
      "Sum of loss at epoch 123: \t 1.2419456541538239\n",
      "Sum of loss at epoch 124: \t 1.2419401854276657\n",
      "Sum of loss at epoch 125: \t 1.2419372349977493\n",
      "Sum of loss at epoch 126: \t 1.2419324666261673\n",
      "Sum of loss at epoch 127: \t 1.241929292678833\n",
      "Sum of loss at epoch 128: \t 1.241929441690445\n",
      "Sum of loss at epoch 129: \t 1.2419231683015823\n",
      "Sum of loss at epoch 130: \t 1.2419214993715286\n",
      "Sum of loss at epoch 131: \t 1.241918370127678\n",
      "Sum of loss at epoch 132: \t 1.2419152706861496\n",
      "Sum of loss at epoch 133: \t 1.2419133633375168\n",
      "Sum of loss at epoch 134: \t 1.241908684372902\n",
      "Sum of loss at epoch 135: \t 1.2419102638959885\n",
      "Sum of loss at epoch 136: \t 1.2419048696756363\n",
      "Sum of loss at epoch 137: \t 1.24190291762352\n",
      "Sum of loss at epoch 138: \t 1.2418993413448334\n",
      "Sum of loss at epoch 139: \t 1.2418958395719528\n",
      "Sum of loss at epoch 140: \t 1.241892397403717\n",
      "Sum of loss at epoch 141: \t 1.2418912500143051\n",
      "Sum of loss at epoch 142: \t 1.2418875396251678\n",
      "Sum of loss at epoch 143: \t 1.2418871968984604\n",
      "Sum of loss at epoch 144: \t 1.24188694357872\n",
      "Sum of loss at epoch 145: \t 1.2418843358755112\n",
      "Sum of loss at epoch 146: \t 1.241878479719162\n",
      "Sum of loss at epoch 147: \t 1.2418738901615143\n",
      "Sum of loss at epoch 148: \t 1.241869568824768\n",
      "Sum of loss at epoch 149: \t 1.241864949464798\n",
      "Sum of loss at epoch 150: \t 1.241862490773201\n",
      "Sum of loss at epoch 151: \t 1.2418576925992966\n",
      "Sum of loss at epoch 152: \t 1.241857722401619\n",
      "Sum of loss at epoch 153: \t 1.2418560683727264\n",
      "Sum of loss at epoch 154: \t 1.241854876279831\n",
      "Sum of loss at epoch 155: \t 1.2418525367975235\n",
      "Sum of loss at epoch 156: \t 1.2418502122163773\n",
      "Sum of loss at epoch 157: \t 1.2418470531702042\n",
      "Sum of loss at epoch 158: \t 1.241843119263649\n",
      "Sum of loss at epoch 159: \t 1.241840049624443\n",
      "Sum of loss at epoch 160: \t 1.2418373674154282\n",
      "Sum of loss at epoch 161: \t 1.2418352961540222\n",
      "Sum of loss at epoch 162: \t 1.2418311834335327\n",
      "Sum of loss at epoch 163: \t 1.2418285757303238\n",
      "Sum of loss at epoch 164: \t 1.2418264895677567\n",
      "Sum of loss at epoch 165: \t 1.2418230921030045\n",
      "Sum of loss at epoch 166: \t 1.2418339252471924\n",
      "Sum of loss at epoch 167: \t 1.2418285310268402\n",
      "Sum of loss at epoch 168: \t 1.2418283224105835\n",
      "Sum of loss at epoch 169: \t 1.2418249547481537\n",
      "Sum of loss at epoch 170: \t 1.2418200224637985\n",
      "Sum of loss at epoch 171: \t 1.2418198585510254\n",
      "Sum of loss at epoch 172: \t 1.2418123930692673\n",
      "Sum of loss at epoch 173: \t 1.2418076395988464\n",
      "Sum of loss at epoch 174: \t 1.2418061643838882\n",
      "Sum of loss at epoch 175: \t 1.241802141070366\n",
      "Sum of loss at epoch 176: \t 1.2417989522218704\n",
      "Sum of loss at epoch 177: \t 1.2417924404144287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 178: \t 1.2417896836996078\n",
      "Sum of loss at epoch 179: \t 1.2417851686477661\n",
      "Sum of loss at epoch 180: \t 1.2417821884155273\n",
      "Sum of loss at epoch 181: \t 1.241778388619423\n",
      "Sum of loss at epoch 182: \t 1.2417745888233185\n",
      "Sum of loss at epoch 183: \t 1.2417714446783066\n",
      "Sum of loss at epoch 184: \t 1.241768404841423\n",
      "Sum of loss at epoch 185: \t 1.2417666018009186\n",
      "Sum of loss at epoch 186: \t 1.2417628318071365\n",
      "Sum of loss at epoch 187: \t 1.241764321923256\n",
      "Sum of loss at epoch 188: \t 1.2417592406272888\n",
      "Sum of loss at epoch 189: \t 1.2417551279067993\n",
      "Sum of loss at epoch 190: \t 1.2417535185813904\n",
      "Sum of loss at epoch 191: \t 1.2417486160993576\n",
      "Sum of loss at epoch 192: \t 1.2417492270469666\n",
      "Sum of loss at epoch 193: \t 1.2417438477277756\n",
      "Sum of loss at epoch 194: \t 1.2417470663785934\n",
      "Sum of loss at epoch 195: \t 1.241738721728325\n",
      "Sum of loss at epoch 196: \t 1.2417402565479279\n",
      "Sum of loss at epoch 197: \t 1.2417334765195847\n",
      "Sum of loss at epoch 198: \t 1.2417343705892563\n",
      "Sum of loss at epoch 199: \t 1.241728737950325\n",
      "Sum of loss at epoch 200: \t 1.241722658276558\n",
      "Sum of loss at epoch 201: \t 1.241719901561737\n",
      "Sum of loss at epoch 202: \t 1.2417184859514236\n",
      "Sum of loss at epoch 203: \t 1.2417187243700027\n",
      "Sum of loss at epoch 204: \t 1.2417185306549072\n",
      "Sum of loss at epoch 205: \t 1.2417133152484894\n",
      "Sum of loss at epoch 206: \t 1.2417088001966476\n",
      "Sum of loss at epoch 207: \t 1.2417074739933014\n",
      "Sum of loss at epoch 208: \t 1.2417020499706268\n",
      "Sum of loss at epoch 209: \t 1.2416995763778687\n",
      "Sum of loss at epoch 210: \t 1.2416957765817642\n",
      "Sum of loss at epoch 211: \t 1.2416942417621613\n",
      "Sum of loss at epoch 212: \t 1.2416911274194717\n",
      "Sum of loss at epoch 213: \t 1.2416943162679672\n",
      "Sum of loss at epoch 214: \t 1.2416853308677673\n",
      "Sum of loss at epoch 215: \t 1.241687148809433\n",
      "Sum of loss at epoch 216: \t 1.2416804879903793\n",
      "Sum of loss at epoch 217: \t 1.241677701473236\n",
      "Sum of loss at epoch 218: \t 1.2416743338108063\n",
      "Sum of loss at epoch 219: \t 1.241669774055481\n",
      "Sum of loss at epoch 220: \t 1.2416660338640213\n",
      "Sum of loss at epoch 221: \t 1.2416630387306213\n",
      "Sum of loss at epoch 222: \t 1.2416586130857468\n",
      "Sum of loss at epoch 223: \t 1.2416549623012543\n",
      "Sum of loss at epoch 224: \t 1.241660788655281\n",
      "Sum of loss at epoch 225: \t 1.241653949022293\n",
      "Sum of loss at epoch 226: \t 1.2416533380746841\n",
      "Sum of loss at epoch 227: \t 1.241646185517311\n",
      "Sum of loss at epoch 228: \t 1.2416588068008423\n",
      "Sum of loss at epoch 229: \t 1.2416556626558304\n",
      "Sum of loss at epoch 230: \t 1.2416566610336304\n",
      "Sum of loss at epoch 231: \t 1.2416517734527588\n",
      "Sum of loss at epoch 232: \t 1.2416505962610245\n",
      "Sum of loss at epoch 233: \t 1.2416437864303589\n",
      "Sum of loss at epoch 234: \t 1.2416450381278992\n",
      "Sum of loss at epoch 235: \t 1.2416297495365143\n",
      "Sum of loss at epoch 236: \t 1.2416373640298843\n",
      "Sum of loss at epoch 237: \t 1.241624653339386\n",
      "Sum of loss at epoch 238: \t 1.2416173219680786\n",
      "Sum of loss at epoch 239: \t 1.241618573665619\n",
      "Sum of loss at epoch 240: \t 1.2416187226772308\n",
      "Sum of loss at epoch 241: \t 1.2416135370731354\n",
      "Sum of loss at epoch 242: \t 1.2416159808635712\n",
      "Sum of loss at epoch 243: \t 1.2416050136089325\n",
      "Sum of loss at epoch 244: \t 1.2416037023067474\n",
      "Sum of loss at epoch 245: \t 1.2415924817323685\n",
      "Sum of loss at epoch 246: \t 1.2415921688079834\n",
      "Sum of loss at epoch 247: \t 1.2415823638439178\n",
      "Sum of loss at epoch 248: \t 1.2415795773267746\n",
      "Sum of loss at epoch 249: \t 1.2415740936994553\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "basicModel = trainMSE(basicModel,train_input, train_target, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  44.1 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "res = evaluateFinalOutput(basicModel,test_input,test_target,mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cross-entropy loss at the classification level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Distinction between `clone()` and `copy_()`\n",
    "Both functions allow to create a clone of a Tensor instance. The difference is that using `clone()`, this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor!\n",
    "Therefore `copy_()` allows to fully separate the cloned tensor from its original version and makes it independent.\n",
    "\n",
    "Check [the doc](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone) for more details (`copy_` is right below `clone`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train each submodule individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassIdentifier(model, train_input, train_classes, mini_batch_size):\n",
    "    \n",
    "    # Specify the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the number of epochs to train the network\n",
    "    epochs = 25\n",
    "    \n",
    "    # Set the learning rate\n",
    "    eta = 0.1\n",
    "\n",
    "    for e in range(epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            model.zero_grad()\n",
    "            for i in range(2):\n",
    "                # For each image (since there are 2 channels)\n",
    "                output = model(train_input[:,i].view(-1,1,14,14).narrow(0, b, mini_batch_size)) # dim,start,length\n",
    "                loss = criterion(output, train_classes[:,i].narrow(0, b, mini_batch_size))\n",
    "                sum_loss += loss.item()\n",
    "                loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        #return\n",
    "        print('Sum of loss at epoch {}: \\t'.format(e),sum_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network's performance with winner takes it all approach\n",
    "def evaluateClassIdentification(model, test_input, test_classes, mini_batch_size):\n",
    "    error = 0\n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input[:,0].view(-1,1,14,14).narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        c_array = output.argmax(1)\n",
    "        t_array = test_classes[:,0][b:b+mini_batch_size]\n",
    "        error += (c_array-t_array).nonzero().size()[0]\n",
    "        \n",
    "    return error/test_input.size()[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mini_batch size\n",
    "mini_batch_size = 200\n",
    "\n",
    "classModel = Parallel_Net()\n",
    "analyzerModel = Analyzer_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of loss at epoch 0: \t 8981.863760471344\n",
      "Sum of loss at epoch 1: \t 23.064595460891724\n",
      "Sum of loss at epoch 2: \t 23.0399227142334\n",
      "Sum of loss at epoch 3: \t 23.023870944976807\n",
      "Sum of loss at epoch 4: \t 23.011502027511597\n",
      "Sum of loss at epoch 5: \t 23.002253532409668\n",
      "Sum of loss at epoch 6: \t 22.995519399642944\n",
      "Sum of loss at epoch 7: \t 22.990675687789917\n",
      "Sum of loss at epoch 8: \t 22.987163066864014\n",
      "Sum of loss at epoch 9: \t 22.984808444976807\n",
      "Sum of loss at epoch 10: \t 22.98237133026123\n",
      "Sum of loss at epoch 11: \t 22.98059105873108\n",
      "Sum of loss at epoch 12: \t 22.979124069213867\n",
      "Sum of loss at epoch 13: \t 22.977797508239746\n",
      "Sum of loss at epoch 14: \t 22.97651767730713\n",
      "Sum of loss at epoch 15: \t 22.975112676620483\n",
      "Sum of loss at epoch 16: \t 22.973365783691406\n",
      "Sum of loss at epoch 17: \t 22.97092890739441\n",
      "Sum of loss at epoch 18: \t 22.967097282409668\n",
      "Sum of loss at epoch 19: \t 22.84385061264038\n",
      "Sum of loss at epoch 20: \t 26.130738258361816\n",
      "Sum of loss at epoch 21: \t 23.0201256275177\n",
      "Sum of loss at epoch 22: \t 23.013102769851685\n",
      "Sum of loss at epoch 23: \t 23.007354974746704\n",
      "Sum of loss at epoch 24: \t 23.002644062042236\n"
     ]
    }
   ],
   "source": [
    "classModel = trainClassIdentifier(classModel, train_input, train_classes, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the model:  90.8 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the class identification of the model\n",
    "res = evaluateClassIdentification(classModel, test_input, test_classes, mini_batch_size)\n",
    "print('Error rate of the model: ',res,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
